---
title: 페이지 교체 정책
date: 2024-12-09 20:00:00 +09:00
categories: [OS, Virtualization]
math: true
tags:
  [operating system, ostep, 페이징, 페이지 교체 정책]
---

운영체제는 페이지 폴트가 발생하면 비어 있는 페이지를 찾아 해당 페이지에 할당한다. 그러나 메모리가 가득 찬 상태라면 운영체제는 페이지 교체 정책(replacement policy)을 사용하여 기존의 페이지 중 일부를 교체한다. 이 글에서는 캐시 관리의 개념과 다양한 페이지 교체 정책, 그리고 각 정책의 성능 차이를 살펴본다.

## **캐시 관리와 AMAT**
메인 메모리는 가상 메모리 페이지를 임시 저장하는 **캐시**로 볼 수 있다. 운영체제는 이 캐시를 효율적으로 관리하여 캐시 미스를 줄이고 캐시 히트를 최대화하려 한다. 이는 결국 디스크 접근 횟수를 최소화하는 데 초점이 맞춰져 있다.

### **평균 메모리 접근 시간 (AMAT)**
캐시 히트와 미스 확률을 활용하면 **평균 메모리 접근 시간**을 계산할 수 있다.

$$
AMAT = (PHit \cdot TM) + (PMiss \cdot TD)
$$

- **PHit**: 캐시 히트 확률
- **PMiss**: 캐시 미스 확률 (1 - PHit)
- **TM**: 메모리 접근 시간
- **TD**: 디스크 접근 시간

#### 예시
컴퓨터가 90%의 히트율을 가지며, 메모리 접근 비용(TM)이 100ns, 디스크 접근 비용(TD)이 10μs라고 가정하면, AMAT는 다음과 같이 계산된다.

$$
AMAT = (0.9 \cdot 100ns) + (0.1 \cdot 10\mu s) = 1.09\mu s
$$

히트율이 99.9%로 증가할 경우 AMAT는 약 **101ns**로 크게 줄어든다. 이는 디스크 접근 비용이 크기 때문에, 작은 미스율 차이가 전체 성능에 큰 영향을 미친다는 것을 보여준다.

---

## **페이지 교체 정책**

### 1. **최적 교체 정책 (OPT)**

최적 교체 정책은 교체 시점에 메모리에 있는 페이지 중 가장 나중에 사용될 페이지를 선택해 미스를 최소화하는 방식이다. 그러나 미래의 접근 패턴을 알 수 없으므로, 다른 정책들의 성능을 평가하는 기준으로만 사용된다.

#### 예제
![alt text](/assets/img/OS/페이징%20교체%20정책/image.png)
_최적 교체 정책의 흐름_

프로그램이 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1의 순서로 페이지를 참조하며, 캐시가 최대 3개의 페이지를 저장할 수 있다고 가정한다.

1. 초기에는 캐시가 비어 있어 첫 3개의 접근은 **시작 미스(cold-start miss)**가 발생한다.
2. 이후 0과 1은 캐시에 남아 있어 히트가 발생한다.
3. 페이지 3 요청 시 가장 나중에 사용될 페이지인 2를 교체한다.
4. 최종 히트율은 $$ \frac{6}{11} \approx 54.5\% $$가 된다.

### 2. **FIFO (선입선출)**

FIFO는 가장 먼저 메모리에 들어온 페이지를 교체하는 간단하고 직관적인 방식으로, 구현이 쉽다는 장점이 있지만, 중요도가 높은 오래된 페이지도 교체될 수 있어 비효율적일 수 있다.


#### 예제
![alt text](/assets/img/OS/페이징%20교체%20정책/image-1.png)
_FIFO 정책의 흐름_
위와 동일한 참조 흐름에서 FIFO는 다음과 같이 동작한다.

1. 첫 3개의 페이지(0, 1, 2)를 순서대로 캐시에 저장한다.
2. 페이지 3 요청 시 가장 먼저 들어온 페이지(0)를 교체한다.
3. 이후 페이지 0이 다시 요청되면서 미스가 발생한다.
4. 최종 히트율은 $$ \frac{4}{11} \approx 36.4\% $$가 된다.

### 3. **무작위 선택**

무작위 선택 방식은 교체할 페이지를 임의로 선택하는 방법으로, 간단한 구현이 가능하고 특정 상황에서는 좋은 성능을 보일 수도 있지만, 중요 페이지를 고려하지 않아 성능이 불안정하다는 단점이 있다.

#### 예제
![alt text](/assets/img/OS/페이징%20교체%20정책/image-3.png)
_무작위 선택 정책의 흐름_

![alt text](/assets/img/OS/페이징%20교체%20정책/image-2.png){: style="width: 70%; display: block; margin: 0 auto;" }
_10,000번 실행할 때의 무작위 선택 정책의 성능_

10,000번의 무작위 선택 시뮬레이션 결과, 최적 정책과 동일한 성능을 보이는 경우도 있었으나, 최악의 경우 매우 낮은 히트율을 기록하기도 한다. 랜덤 알고리즘은 안정적이지 않다.


### 4. **LRU (Least Recently Used)**

LRU는 가장 오래 사용되지 않은 페이지를 교체하는 방식으로, 과거의 접근 패턴을 활용해 최적 정책에 가까운 성능을 보이는 장점이 있지만, 구현이 복잡하고 메모리 오버헤드가 발생할 수 있다는 단점이 있다.

#### 예제
![alt text](/assets/img/OS/페이징%20교체%20정책/image-4.png)
_LRU 정책의 흐름_

- LRU는 과거 접근 기록을 활용하여 페이지 2를 먼저 교체한 뒤 이후 참조에서 히트를 발생시킨다.
- 최종 히트율은 최적 정책과 동일한 $$ \frac{6}{11} \approx 54.5\% $$가 된다.

---

## **워크로드에 따른 성능 비교**

### 1. 지역성이 없는 워크로드
접근되는 페이지들의 집합에서 페이지가 무작위적으로 참조되는 워크로드이다. 이 예제에서는 100개의 페이지들이 일정 시간 동안 계속 접근하는 워크로드를 사용한다. 접근되는 페이지는
무작위적으로 선택되며 페이지들이 총 10,000번 접근된다. 실험에서는 캐시의 크기를
매우 작은 것부터(한 페이지) 모든 페이지들을 담을 수 있을 정도의 크기까지(100페이지)
증가시켰으며, 이를 통해 각 정책이 캐시 크기에 따라 어떻게 동작하는지 살펴보았다.

![alt text](/assets/img/OS/페이징%20교체%20정책/image-5.png){: style="width: 70%; display: block; margin: 0 auto;" }
_지역성이 없는 워크로드_

위의 그림에서는 최적의 방법과 LRU, 랜덤 그리고 FIFO 방식을 사용하였을 때의
실험 결과를 나타낸다. 그림의 y 축은 각 정책이 달성한 히트율을 나타내며 x 축은 앞서설명한 캐시 크기의 변화를 나타낸다.

#### 결론
- FIFO, LRU, 무작위 선택 모두 비슷한 성능을 보이며, 캐시 크기에 따라 히트율이 결정된다.

### 2. **80/20 워크로드**
20%의 페이지들에서 (“인기 있는” 페이지) 80%의 참조가 발생하고 나머지 80%의 페이지들에 대해서 20%의 참조만 (“비인기” 페이지) 발생한다. 이 워크로드에서는 마찬가지로 총 100개의 페이지 들이 있다. “인기 있는” 페이지들에 대한 참조가 실험 시간의 대부분을 차지한다.

![alt text](/assets/img/OS/페이징%20교체%20정책/image-6.png){: style="width: 70%; display: block; margin: 0 auto;" }
_80 대 20 워크로드_

#### 결론
- LRU가 높은 히트율을 보인다. 지역성의 원칙을 활용해 중요한 페이지를 캐시에 오래 유지하기 때문이다.

### 3. 순차 반복 워크로드

이 워크로드는 50개의 페이지들을 순차적으로 참조한다. 0번 페이지를 참조하고
1번을 참조하고 ..., 49번째의 페이지를 참조한 후에 다시 처음으로 돌아가서 그 접근
순서를 반복한다. 50개의 개별적인 페이지들을 총 10,000번 접근한다.

![alt text](/assets/img/OS/페이징%20교체%20정책/image-7.png){: style="width: 70%; display: block; margin: 0 auto;" }
_순환 형 워크로드_

#### 결론
- FIFO와 LRU는 낮은 성능을 보이며, 무작위 선택이 의외로 높은 성능을 보인다.

---

## **LRU 정책 근사하기**

정확한 LRU 구현은 어렵기 때문에, LRU를 근사하는 방법으로 **시계 알고리즘**이 사용된다.

1. 각 페이지에 `use bit`를 추가하여 최근 사용 여부를 기록한다.
2. 교체 시 `use bit`가 0인 페이지를 우선적으로 선택한다.
3. 이 방식을 통해 LRU에 가까운 효율성을 얻는다.

![alt text](/assets/img/OS/페이징%20교체%20정책/image-8.png){: style="width: 70%; display: block; margin: 0 auto;" }
_시계 기법을 사용한 80 대 20 워크로드_

---

## **갱신된 페이지 (Dirty Page) 고려**
운영체제가 페이지 교체 대상을 선택할 때, 메모리에 탑재된 이후 페이지가 변경되었는지를 고려하는 것은 매우 중요하다. 이러한 접근은 Corbato [Cor69]에 의해 처음 제안되었으며, 운영체제가 페이지를 교체할 때 불필요한 디스크 작업을 줄이고 시스템 효율성을 높이는 데 기여한다.

### **Dirty Page와 Clean Page의 차이**
페이지가 변경되어 더티(dirty) 상태가 된 경우, 이를 교체하려면 디스크에 변경 내용을 기록해야 하므로 추가 비용이 발생한다. 반면, 변경되지 않은 깨끗한(clean) 페이지는 디스크에 기록할 필요가 없기 때문에 비용 없이 바로 교체할 수 있다. 따라서 많은 가상 메모리(VM) 시스템은 더티 페이지보다 깨끗한 페이지를 우선적으로 교체하는 것을 선호한다.

### **Modified Bit(더티 비트)**
더티 페이지를 식별하기 위해 하드웨어는 페이지마다 **modified bit(더티 비트)**를 포함한다. 이 비트는 페이지가 변경될 때 자동으로 1로 설정되며, 운영체제는 이를 확인해 페이지가 변경되었는지 여부를 판단한다. 이를 통해 교체 대상을 선택할 때 더티와 깨끗한 페이지를 구분할 수 있다.

### **시계 알고리즘의 개선**
기존 시계 알고리즘은 `use bit`를 사용해 최근에 사용되지 않은 페이지를 교체 대상으로 삼았다. 이를 개선하여 `use bit`와 `modified bit`를 함께 고려하면 다음과 같은 방식으로 작동한다:

1. **사용되지 않은 깨끗한 페이지 우선 선택**: 사용 이력이 없고 깨끗한 페이지를 먼저 교체 대상으로 삼는다.
2. **대안 선택**: 위 조건을 만족하는 페이지가 없다면, 사용되지 않았지만 더티 상태인 페이지를 선택한다. 

이 방식은 디스크 작업을 줄이고, 페이지 교체의 비용을 최소화하는 데 효과적이다. 결과적으로 개선된 시계 알고리즘은 많은 시스템에서 실질적인 성능 향상을 가져온다.

---

## **다른 VM 정책들**
페이지 교체 정책은 가상 메모리 시스템에서 가장 중요한 정책 중 하나이지만, 운영체제는 이외에도 다양한 정책을 채택해 메모리 관리를 최적화한다. 그중 대표적인 것이 **페이지 선택(Page Selection) 정책**이다.

### **요구 페이징(Demand Paging)**
운영체제는 보통 **요구 페이징** 방식을 사용한다. 이 방식은 페이지가 실제로 참조될 때에만 해당 페이지를 메모리로 불러오는 것이다. 이는 불필요한 페이지를 메모리로 읽어오는 것을 방지해 효율성을 높인다.

### **선반입(Prefetching)**
일부 시스템은 특정 페이지가 메모리에 탑재될 때, 연관된 다른 페이지도 미리 읽어오는 **선반입(prefetching)** 방식을 사용하기도 한다. 예를 들어, 코드 페이지 P가 메모리에 로드되면, 다음 페이지 P+1이 참조될 확률이 높다고 판단해 이를 미리 메모리에 탑재한다. 이 방식은 선반입된 페이지가 실제로 사용될 경우 큰 성능 향상을 가져올 수 있지만, 성공 확률이 높을 때만 유효하다.

### **클러스터링(Clustering)**
운영체제가 변경된 페이지를 디스크에 기록하는 방식도 중요한 정책 중 하나이다. 일부 시스템은 페이지를 하나씩 디스크에 쓰기보다는, 여러 페이지를 모아서 한 번에 기록하는 **클러스터링(clustering)** 방식을 채택한다. 디스크는 큰 쓰기 작업을 더 효율적으로 처리할 수 있기 때문에, 이 방식은 시스템 성능을 향상시키는 데 효과적이다.

---

## **쓰래싱(Thrashing)과 해결 방안**

### **쓰래싱(Thrashing) 문제**
쓰래싱은 메모리 사용 요구가 물리 메모리를 초과하여 운영체제가 끊임없이 페이징 작업을 수행하는 상태를 말한다. 이로 인해 CPU는 대부분의 시간을 페이지를 교체하는 데 사용하게 되며, 결과적으로 시스템 성능이 급격히 저하된다.

### **초기 운영체제의 해결 방법**
초기의 많은 운영체제는 쓰래싱 문제를 해결하기 위해 **진입 제어(admission control)**라는 방법을 사용했다. 이 방식은 실행 중인 프로세스 수를 줄여, 남은 프로세스들이 필요한 모든 페이지를 메모리에 올릴 수 있도록 한다. 

- **워킹 셋(Working Set)**: 프로세스가 일정 시간 동안 사용하는 페이지들의 집합을 워킹 셋이라고 하며, 이를 기준으로 실행할 프로세스 수를 조정한다.
- **진입 제어의 장점**: 적은 프로세스를 메모리에 탑재함으로써 각 프로세스의 성능을 보장하고, 쓰래싱을 방지할 수 있다.

### **현대 시스템의 대처 방법**
현대 운영체제는 보다 과감한 조치를 취하기도 한다. 예를 들어, 일부 Linux 버전에서는 **메모리 부족 킬러(Out-of-Memory Killer)**라는 데몬을 실행한다. 

- **작동 방식**: 메모리 부족 상태에서 가장 많은 메모리를 소비하는 프로세스를 종료한다.
- **문제점**: 이 방식은 메모리를 확보하는 데는 효과적이지만, 중요한 프로세스(X 서버 등)가 종료될 경우 다른 응용 프로그램들이 함께 중단되는 문제가 발생할 수 있다.
