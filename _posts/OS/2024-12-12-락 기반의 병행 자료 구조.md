---
title: 락 기반의 병행 자료 구조
date: 2024-12-12 20:00:00 +09:00
categories: [OS, Concurrency]
math: true
tags:
  [operating system, ostep, concurrency, lock, data structure]
---
병행성(Concurrency)을 구현하기 위해 **상호 배제(Mutual Exclusion)**가 필수적이며, 이를 구현하는 데 **락(Lock)**이 사용된다.

락은 흔히 사용되는 자료 구조에 추가하여 여러 스레드가 안전하게 사용할 수 있도록 만들어 준다. 락이 적용된 자료 구조는 **스레드 안전(thread-safe)**하다고 표현한다. 이 글에서는 스레드 안전한 여러 자료 구조들을 구현하는 방법을 살펴 본다.

## **Concurrent Counters(병행 카운터)**

### 1. 확장성 없는 카운터

```c
typedef struct _ _counter_t {
int value;
pthread_mutex_t lock;
} counter_t;

void init(counter_t *c) {
c−>value = 0;
Pthread_mutex_init(&c−>lock , NULL);
}

void increment(counter_t *c) {
Pthread_mutex_lock(&c−>lock);
c−>value++;
Pthread_mutex_unlock(&c−>lock);
}

void decrement(counter_t *c) {
Pthread_mutex_lock(&c−>lock);
c−>value−−;
Pthread_mutex_unlock(&c−>lock);
}

int get(counter_t *c) {
Pthread_mutex_lock(&c−>lock);
int rc = c−>value;
Pthread_mutex_unlock(&c−>lock);
return rc;
}
```

위와 같이 카운터에 접근, 조작하는 구역은 Lock으로 감싸 thread safe한 카운터를 구현할 수 있다.

#### 성능 평가

![alt text](/assets/img/OS/락%20기반%20병행%20자료구조/image.png)
_Precise vs Sloppy_

각 스레드가 특정 횟수만큼 **공유 카운터**를 증가시키는 벤치마크를 실행하면, **확장성이 없는 카운터**에서는 스레드 수가 증가할수록 실행 시간이 점점 더 오래 걸리는 것을 확인할 수 있다.

이는 **병목 현상** 때문으로, 카운터와 락이 단 하나만 존재하기 때문에 발생한다.  
하나의 스레드가 카운터를 조정하기 위해 락을 획득하는 동안, 나머지 스레드들은 락을 획득할 수 있을 때까지 대기해야 한다. 이렇게 여러 스레드가 락을 기다리며 작업이 지연되는 상황을 **병목 현상**이라고 한다.


> 완벽한 확장성: 더 많은 작업을 처리하
더라도 각 작업이 병렬적으로 처리되어 완료 시간이 늘어나지 않는다는 것
{: .prompt-tip }

### **2. 확장성 있는 카운터 (Sloppy Counter)**

```c
typedef struct _counter_t {
    int global;                  // 전역 카운터
    pthread_mutex_t glock;       // 전역 락
    int local[NUMCPUS];          // CPU별 지역 카운터
    pthread_mutex_t llock[NUMCPUS]; // CPU별 지역 락
    int threshold;               // 한계치 S
} counter_t;

// init:한계치를 기록하고 락과 지역카운터
// 그리고 전역 카운터의 값들을 초기화함
void init(counter_t *c, int threshold) {
    c->threshold = threshold;
    c->global = 0;
    pthread_mutex_init(&c->glock, NULL);

    for (int i = 0; i < NUMCPUS; i++) {
        c->local[i] = 0;
        pthread_mutex_init(&c->llock[i], NULL);
    }
}


// update:  보통은 지역 락을 획득한 후 지역 값을 갱신함
//          '한계치'까지 지역 카운터의 값이 커지면,
//          전역 락을 획득하고 지역 값을 전역 카운터에 전달함
void update(counter_t *c, int threadID, int amt) {
    pthread_mutex_lock(&c->llock[threadID]);
    c->local[threadID] += amt;  // 지역 카운터 갱신

    if (c->local[threadID] >= c->threshold) { // 한계치 초과 시
        pthread_mutex_lock(&c->glock);
        c->global += c->local[threadID];      // 전역 카운터로 전달
        pthread_mutex_unlock(&c->glock);
        c->local[threadID] = 0;              // 지역 카운터 초기화
    }
    pthread_mutex_unlock(&c->llock[threadID]);
}

// get: 전역 카운터의 값을 리턴(정확하지 않을 수 있음)
int get(counter_t *c) {
    pthread_mutex_lock(&c->glock);
    int val = c->global; // 전역 카운터 값 반환
    pthread_mutex_unlock(&c->glock);
    return val;
}
```

#### **개념**
**확장성 있는 카운터**는 하나의 논리적 카운터를 **CPU별 지역 카운터(Local Counter)**와 **전역 카운터(Global Counter)**로 나누어 구현한 방식이다.  
- CPU마다 **지역 카운터**와 **지역 락(Local Lock)**이 존재하며, 카운터 갱신 작업은 주로 이 지역 카운터를 통해 이루어진다.
- 지역 카운터는 일정 주기마다 전역 카운터로 값이 전달된다. 이때 **전역 락(Global Lock)**을 사용하여 지역 카운터 값을 전역 카운터에 더하고, 지역 카운터는 초기화된다.

#### **작동 원리**
1. **카운터 갱신**:
   - 각 스레드는 **지역 카운터**를 갱신한다. 지역 카운터는 지역 락에 의해 보호되며, CPU별로 분산되어 경쟁이 적다.
   - 카운터 갱신은 확장성이 뛰어나며, 병목 현상을 줄인다.

2. **전역 갱신**:
   - 지역 카운터가 특정 한계치(`S`, sloppiness)를 초과하면, 해당 값은 **전역 카운터**로 전달된다.
   - 이 과정은 전역 락을 사용하며, 지역 카운터는 초기화된다.

3. **정확성 조정**:
   - 전역 카운터의 값은 정확성을 완전히 보장하지 않는다.  
   - `S` 값을 조정하여 성능과 정확성의 균형을 맞출 수 있다.  
     - 작은 `S`: 높은 정확도, 낮은 성능.
     - 큰 `S`: 높은 성능, 낮은 정확도.

![alt text](/assets/img/OS/락%20기반%20병행%20자료구조/image-1.png)
_엉성한 카운터의 확장성_

확장성 있는 카운터(Sloppy Counter)는 다수의 스레드와 CPU 환경에서 뛰어난 성능을 제공한다.
정확성과 성능 간의 균형을 조절할 수 있어 다양한 상황에서 유연하게 활용할 수 있는 방식이다.

---

## **Concurrent Linked-Lists(병행 연결 리스트)**

```c
// 노드 구조체 정의
typedef struct _node_t {
    int key;               // 노드의 키 값
    struct _node_t *next;  // 다음 노드 포인터
} node_t;

// 리스트 구조체 정의
typedef struct _list_t {
    node_t *head;           // 리스트의 헤드 노드
    pthread_mutex_t lock;   // 리스트 보호를 위한 락
} list_t;

// 리스트 초기화 함수
void List_Init(list_t *L) {
    L->head = NULL;  // 헤드 노드 초기화
    pthread_mutex_init(&L->lock, NULL);  // 락 초기화
}

// 리스트에 삽입 함수
int List_Insert(list_t *L, int key) {
    pthread_mutex_lock(&L->lock);  // 리스트 락 획득

    // 새로운 노드 메모리 할당
    node_t *new = malloc(sizeof(node_t));
    if (new == NULL) {  // 메모리 할당 실패 시 처리
        perror("malloc");
        pthread_mutex_unlock(&L->lock);  // 락 해제
        return -1;
    }

    // 노드 초기화 및 리스트 삽입
    new->key = key;
    new->next = L->head;
    L->head = new;

    pthread_mutex_unlock(&L->lock);  // 락 해제
    return 0;  // 성공
}

// 리스트에서 키 검색 함수
int List_Lookup(list_t *L, int key) {
    pthread_mutex_lock(&L->lock);  // 리스트 락 획득

    node_t *curr = L->head;
    while (curr) {  // 리스트 탐색
        if (curr->key == key) {  // 키가 발견되면
            pthread_mutex_unlock(&L->lock);  // 락 해제
            return 0;  // 성공
        }
        curr = curr->next;
    }

    pthread_mutex_unlock(&L->lock);  // 락 해제
    return -1;  // 실패 (키를 찾지 못함)
}
```

### **코드 설명**

#### 1. `List_Init`
- 리스트를 초기화하는 함수이다.
- 헤드 포인터를 `NULL`로 설정하며, 리스트 보호를 위한 락(`pthread_mutex_t`)을 초기화한다.

#### 2. `List_Insert`
- 새로운 노드를 생성하여 리스트의 헤드에 삽입하는 함수이다.
- 리스트에 쓰레드 안전성을 보장하기 위해 락을 사용한다.
- 메모리 할당 실패 시 오류 메시지를 출력하고 -1을 반환한다.
- 삽입이 성공하면 0을 반환한다.

#### 3. `List_Lookup`
- 리스트를 순회하며 특정 키 값을 검색하는 함수이다.
- 키가 발견되면 0을 반환하고, 키를 찾지 못하면 -1을 반환한다.
- 검색 중 리스트의 쓰레드 안전성을 보장하기 위해 락을 사용한다.

---

### **특징**

1. **쓰레드 안전(Thread-Safe)**:
   - 락(`pthread_mutex_t`)을 사용하여 여러 스레드가 동시에 리스트를 접근해도 데이터 무결성을 보장한다.

2. **단순한 구현**:
   - 삽입과 검색 모두 직관적인 알고리즘으로 작성되었다.
   - 삽입은 O(1), 검색은 O(n)의 시간 복잡도를 갖는다.

3. **확장성 한계**:
   - 단일 락을 사용하므로 리스트가 커지거나 다수의 스레드가 접근할 경우 병목 현상이 발생할 수 있다.
   - 병목 문제를 해결하기 위해 더 정교한 락 메커니즘(예: 분리된 락)을 고려할 수 있다.

위와 같이 병행 연결 리스트를 구현하면, 리스트의 삽입과 탐색 과정에서 락을 사용하여 **쓰레드 안전(thread-safe)**한 리스트를 만들 수 있다. 그러나 하나의 락만 사용하기 때문에 리스트에 접근하는 스레드 수가 많아질수록 병목 현상이 발생하며, 확장성이 떨어진다.

이러한 확장성 문제를 해결하기 위해 hand-over-hand locking 기법을 사용할 수 있다. 이 방법은 각 노드에 개별적인 락을 추가하여, 리스트를 순회하거나 연산할 때 노드별로 락을 획득하고 해제하는 방식이다.

그러나 hand-over-hand locking은 병행성을 높이는 것처럼 보이지만, 리스트를 순회할 때 각 노드마다 락을 획득하고 해제하는 오버헤드가 크기 때문에 성능 향상이 제한적이다. 이러한 오버헤드는 특히 리스트가 길거나 스레드 수가 많은 환경에서 문제가 될 수 있다.

> 자료 구조를 설계할 때, 락 획득과 해제 같은 부하가 큰 연산을 추가했다면 병행성이 좋아졌다는 사실만으로는 큰 의미가 없다.
오히려, 부하가 적은 간단한 설계가 더 나은 성능을 제공할 수 있다. 복잡한 락 구조를 추가하고 병행성을 높이려는 시도가 성능 저하로 이어질 수도 있기 때문이다.

복잡한 설계와 높은 병행성이 항상 좋은 결과를 보장하지 않는다. 성능을 평가하려면 간단하지만 병행성이 낮은 방법과 복잡하지만 병행성이 높은 방법 둘 다 구현하고 실제로 성능을 측정해야 한다.
{: .prompt-tip }

---

## **Concurrent Queues(병행 큐)**

```c

// 노드 구조체 정의
typedef struct _node_t {
    int value;            // 노드의 값
    struct _node_t *next; // 다음 노드를 가리키는 포인터
} node_t;

// 큐 구조체 정의
typedef struct _queue_t {
    node_t *head;                // 큐의 헤드 포인터
    node_t *tail;                // 큐의 테일 포인터
    pthread_mutex_t headLock;    // 헤드 보호 락
    pthread_mutex_t tailLock;    // 테일 보호 락
} queue_t;

// 큐 초기화 함수
void Queue_Init(queue_t *q) {
    node_t *tmp = malloc(sizeof(node_t)); // 더미 노드 생성
    assert(tmp != NULL);
    tmp->next = NULL;

    q->head = q->tail = tmp; // 헤드와 테일을 더미 노드로 초기화
    pthread_mutex_init(&q->headLock, NULL); // 헤드 락 초기화
    pthread_mutex_init(&q->tailLock, NULL); // 테일 락 초기화
}

// 큐에 값 추가 (Enqueue)
void Queue_Enqueue(queue_t *q, int value) {
    node_t *tmp = malloc(sizeof(node_t)); // 새로운 노드 생성
    assert(tmp != NULL);
    tmp->value = value;
    tmp->next = NULL;

    pthread_mutex_lock(&q->tailLock); // 테일 락 획득
    q->tail->next = tmp;              // 테일에 새 노드 연결
    q->tail = tmp;                    // 테일 포인터 갱신
    pthread_mutex_unlock(&q->tailLock); // 테일 락 해제
}

// 큐에서 값 제거 (Dequeue)
int Queue_Dequeue(queue_t *q, int *value) {
    pthread_mutex_lock(&q->headLock); // 헤드 락 획득

    node_t *tmp = q->head;
    node_t *newHead = tmp->next; // 새로운 헤드 노드
    if (newHead == NULL) {       // 큐가 비어 있는 경우
        pthread_mutex_unlock(&q->headLock); // 헤드 락 해제
        return -1;                          // 실패
    }

    *value = newHead->value;  // 제거된 노드의 값 반환
    q->head = newHead;        // 헤드 포인터 갱신
    pthread_mutex_unlock(&q->headLock); // 헤드 락 해제

    free(tmp); // 기존 헤드 노드 메모리 해제
    return 0;  // 성공
}

```

### **큐와 락의 사용**

이 코드는 **헤드 락**과 **테일 락**이라는 두 개의 락을 사용하여 큐에 삽입과 추출 연산에서 **병행성**을 제공하는 구조이다.  
- **삽입 연산**은 테일 락을 사용하여 큐의 끝에 노드를 추가하도록 설계되어 있다.  
- **추출 연산**은 헤드 락을 사용하여 큐의 앞에서 노드를 제거하도록 설계되어 있다.  

일반적인 경우, 삽입 연산은 **테일 락**만, 추출 연산은 **헤드 락**만 접근하므로 두 연산이 독립적으로 동작할 수 있다.

### **더미 노드의 역할**

Michael과 Scott의 큐 구현에서는 초기화 시 **더미(dummy) 노드**를 추가하는 방법을 사용하였다.  
- 이 더미 노드는 큐의 **헤드와 테일 연산을 구분**하는 데 사용된다.  
- 더미 노드를 통해 삽입과 추출 연산이 충돌하지 않도록 설계되었다.

### **큐의 한계와 확장성**

현재 구현된 큐는 멀티 스레드 환경에서 기본적인 삽입과 추출 연산을 지원하지만, 몇 가지 한계가 존재한다:
1. **대기 기능 부족**  
   - 큐가 비어 있거나 가득 찬 상태에서 스레드가 대기하도록 설계되지 않았다.  
   - 이러한 기능이 없는 큐는 복잡한 멀티 스레드 프로그램에서 사용하기 어렵다.

2. **유한 큐**  
   - 유한한 크기를 가진 큐를 구현하려면, 큐가 가득 찬 경우 **삽입을 대기**시키고, 비어 있는 경우 **추출을 대기**시키는 메커니즘이 필요하다.  
   - 이를 구현하려면 **조건 변수(Condition Variable)**와 같은 추가적인 동기화 도구가 필요하다.

## **Concurrent Hash Tables(병행 테이블)**

```c
#define BUCKETS ()

// 해시 테이블 구조체
typedef struct _hash_t {
    list_t lists[BUCKETS]; // 버킷으로 연결 리스트 배열 사용
} hash_t;

// 해시 테이블 초기화 함수
void Hash_Init(hash_t *H) {
    for (int i = 0; i < BUCKETS; i++) {
        List_Init(&H->lists[i]); // 각 버킷 리스트 초기화
    }
}

// 해시 테이블 삽입 함수
int Hash_Insert(hash_t *H, int key) {
    int bucket = key % BUCKETS; // 버킷 인덱스 계산
    return List_Insert(&H->lists[bucket], key);
}

// 해시 테이블 탐색 함수
int Hash_Lookup(hash_t *H, int key) {
    int bucket = key % BUCKETS; // 버킷 인덱스 계산
    return List_Lookup(&H->lists[bucket], key);
}
```
### **병행 해시 테이블의 특징**

1. **구조**:
   - **병행 리스트**를 기반으로 구현되었다.
   - **해시 버킷마다 락을 사용**하여 병행성을 향상시켰다.
   - 병렬 연산을 지원하므로 성능이 우수하다.

2. **병행성**:
   - 단일 락을 사용하는 전체 구조가 아니라, **버킷별로 락을 분리**함으로써 여러 스레드가 동시에 다른 버킷에 접근할 수 있다.
   - 이러한 설계는 병목 현상을 줄이고 확장성을 높이는 데 기여한다.

### **성능 비교**

![alt text](/assets/img/OS/락%20기반%20병행%20자료구조/image-2.png)
_확장성이 좋은 해시 테이블_

- **환경**:
  - 4개의 CPU를 갖춘 iMac에서, 4개의 스레드가 각각 10,000에서 50,000개의 갱신 연산을 해시 테이블에 수행하였다.

- **결과**:
  - 병행 해시 테이블은 **확장성이 매우 뛰어나다**.  
    - 각 버킷의 락을 분리하여 스레드 간 병렬성을 극대화하였다.
  - 반면, 단일 락을 사용하는 연결 리스트는 **확장성이 크게 떨어진다**.  
    - 모든 연산이 하나의 락에 의존하므로, 스레드 수가 증가할수록 병목 현상이 심화되었다.



> 병행 자료 구조를 설계할 때는 **하나의 큰 락**을 추가하여 동기화 접근을 제어하는 가장 단순한 방법부터 시작하는 것이 좋다. 이 방법은 우선 **제대로 동작하는 락**을 구현할 수 있도록 해준다. 
성능 저하가 문제가 된다고 판단되면, 그때 최적화를 진행하면 된다. 이렇게 하면 결과적으로 **꼭 필요한 부분만 개선**하여 효율적인 자료 구조를 만들 수 있다. Knuth의 유명한 말처럼, **“미숙한 최적화는 모든 악의 근원이다.”** 기초적인 구현부터 시작해 점진적으로 개선하는 접근법이 가장 효과적이다.
{: .prompt-tip }